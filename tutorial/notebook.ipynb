{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: How Do We Know It's Working?\n",
    "This tutorial is based on the official tutorial titled [Working With Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) by the `scikit-learn` team. Compared to those versions, the current tutorial features a dataset that has been published by [Kaggle](https://www.kaggle.com) as part of a competition that they organised for [SMS Spam Detection](https://www.kaggle.com/c/sms-spam-detection). Furthermore, a number of changes have been carried in the code of the above-mentioned tutorials in order to include `pandas` in the data pre-processing stage, and to assure compatibility with the updated version of `scikit-learn`.\n",
    "\n",
    "In this tutorial you will learn how to:\n",
    "* Extract feature vectors from text documents\n",
    "* Load, inspect and pre-process a dataset of comments on social media\n",
    "* Train a classifier to predict whether an SMS is spam (e.g. \"You are rewarded with a $1500 Bonus Prize, call 09066364589\") or not\n",
    "* Use Grid Search in order to tune better the hyper-parameters of your Machine Learning pipeline\n",
    "* Perform K-Means clustering and explore its results\n",
    "\n",
    "In order to run this iPython Notebook, [Jupyter](http://jupyter.org/) should be installed in your machine. Besides Jupyter, the following Python packages should also be installed: (i) `pandas` and (ii) `scikit-learn`. The easiest way to install all of these together is with [Anaconda](https://www.anaconda.com/) (Windows, macOS and Linux installers available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using this library to detect the Python version with \n",
    "# which the Notebook kernel is running.\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You are running Python %s' % sys.version)\n",
    "version_info_major = sys.version_info.major"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Kaggle Dataset\n",
    "For the purposes of this tutorial, we will be using a dataset of SMS messages along with their classification labels (i.e. \"spam\" or \"ham\"). The dataset is encoded in binary-encoded `pickle` files which reside in `./Kaggle/spam.pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset_location = './Kaggle/spam.pickle' # The location of the Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cPickle should be loaded on in the case that kernel is running\n",
    "# on Python 2.\n",
    "if version_info_major < 3:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the binary-encoded pickle files from the designated location.\n",
    "with open(kaggle_dataset_location, 'rb') as f:\n",
    "    kaggle_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `kaggle_dataset` variable contains the dataset as a pythonic dictionary of lists. We will be using the `pandas` library in order to tranform this structure into a `pandas.DataFrame` which will simplify the data inspection and pre-processing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "kaggle_dataset_df = pd.DataFrame(kaggle_dataset)\n",
    "# Printing the number of rows (data points) in the loaded DataFrame.\n",
    "print('The number of rows in the loaded DataFrame is: %d' % len(kaggle_dataset_df)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first 10 rows of the `DataFrame` in order to get an understanding of the structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(kaggle_dataset_df.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kaggle_dataset_df['Message'][1])\n",
    "print(kaggle_dataset_df['Message'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the available columns of the dataset.\n",
    "print(kaggle_dataset_df.columns)\n",
    "# Define the target column which we want to predict.\n",
    "target_column = u'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the category codes of each of the classes in the target-column.\n",
    "# outputs = kaggle_dataset_df['Class'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = kaggle_dataset_df['Class'].astype('category').cat.categories.tolist()\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to gain a basic understanding about how potentially imbalanced towards certain classes our dataset is before moving further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in class_names:\n",
    "    print('%d comments that are labeled as %s.' % (len(kaggle_dataset_df[kaggle_dataset_df['Class'] == cl]), cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random_state number for replicability of the experiments.\n",
    "random_state = 10\n",
    "resampled_ham_df = kaggle_dataset_df[kaggle_dataset_df['Class'] == 'ham'].sample(len(kaggle_dataset_df[kaggle_dataset_df['Class'] == 'spam']), \n",
    "                                                                                 random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resampled_ham_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_dataset_df = pd.concat([resampled_ham_df, kaggle_dataset_df[kaggle_dataset_df['Class'] == 'spam']],\n",
    "                                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the category codes of each of the classes in the target-column.\n",
    "outputs = resampled_dataset_df['Class'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = resampled_dataset_df['Message']\n",
    "class_names = resampled_dataset_df['Class'].astype('category').cat.categories.tolist()\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in class_names:\n",
    "    print('%d comments that are labeled as %s.' % (len(resampled_dataset_df[resampled_dataset_df['Class'] == cl]), cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the performance of our algorithm, we should test its performance on data that it hasn't *seen* during training. Luckily, `scikit-learn` includes an appropriate function that splits the items for a dataset into random train and test subsets.\n",
    "\n",
    "We set the portion of the original dataset that will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "# Split dataset into training and testing according to the test_size variable.\n",
    "# output_train and output_test are lists containing the classes' indices.\n",
    "input_train, input_test = train_test_split(inputs.tolist(), test_size=test_size, random_state=random_state)\n",
    "y_train, y_test = train_test_split(outputs.tolist(), test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features from Text: The Bag-of-Words Approach\n",
    "In order to be able to use text documents$^1$ as either input to Machine Learning algorithms, we need to follow a process that would turn them into numerical feature vectors. We generally refer to this process as *vectorisation*. The most intuitive way to do so is the **bags-of-words** approach, which is carried out as follows:\n",
    "1. Identify all the words that occur in the documents of a training set.\n",
    "2. Assign a fixed integer ID to each one of those words. For example in Python you could build a dictionary that would map each word to each corresponding integer ID:\n",
    " ```python\n",
    " dictionary = {'I': 1,\n",
    "               'study': 2,\n",
    "               'machine': 3,\n",
    "               'learning': 4,\n",
    "               ...}\n",
    " ```\n",
    "3. For each document in the training set, we count the number of occurrences of each word, and we store it in $X[d, w]$, as the value of the $w$-th feature for the $d$-th document, where $w$ is the index of the word in the dictionary.\n",
    "\n",
    "The bags-of-words representation implies that total number of features is the number of distinct words in the corpus, which typically is larger than 100k. \n",
    "\n",
    "While storing all these values in a `numpy` array would require substantial amount of memory, most values in $X$ will be zeros since for a given document only a small subset of the set of the distinct words in the dataset will be present. For this reason, we say that bags-of-words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory. `scipy.sparse` matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures.\n",
    "\n",
    "In `scipy` text preprocessing, tokenising and stop-words (e.g. \"and\", \"or\" and \"that\") filtering are included in a high-level component that is able to build a dictionary of features and transform documents to feature vectors:\n",
    "\n",
    "$^1$ Text documents can vary substantially in length and writing style. In our case, we refer to text documents as the short-lengthed comments of our Kaggle dataset, but the techniques presented in this tutorial could work on much longer collections, such as articles or books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# The lines below load the TweetTokenizer from the nltk library.\n",
    "# You can comment-them-in along with the tokenizer variable of\n",
    "# the CountVectorizer should you like to see the results with\n",
    "# a different tokeniser.\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# word_tokenizer = TweetTokenizer(preserve_case=False, \n",
    "#                                 strip_handles=True, \n",
    "#                                 reduce_len=True).tokenize\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                                   stop_words=None,\n",
    "                                   # tokenizer=word_tokenizer,\n",
    "                                   # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   max_df=1.0,\n",
    "                                   # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   min_df=1)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on a Toy Example\n",
    "Let’s use it to tokenize and count the word occurrences of a minimalistic corpus of text documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [u'You have studied Machine Learning',\n",
    "              u'We love learning Machine Learning',\n",
    "              u'Looking forward to #ArupLearningWeek',\n",
    "              u'Have you studied Machine Learning']\n",
    "\n",
    "# Fits and tranforms the corpus in its bag-of-words representation.\n",
    "toy_count = count_vectorizer.fit_transform(toy_corpus) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the fitting process each term is assigned a unique integer index corresponding to a column in the resulting `toy_count` matrix (i.e. equivalent to the $X$ matrix that has been mentioned in the description of this section of the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(toy_count) # This is the memory-efficient representation of a sparse matrix.\n",
    "print(toy_count.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of DOCUMENTS in the corpus equals to the number of rows of the returned array: %d' %  toy_count.toarray().shape[0])\n",
    "print('The total number of UNIQUE WORDS in the corpus equals to the number of columns of the returned array: %d' %  toy_count.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the first and the last rows of the array are identical. This is happening because they correspond to comments with the same words, and, thus, are encoded in equal vectors, which leads to loss of valuable information. `CountVectorizer` also supports counts of n-grams of words or consecutive characters. N-grams are runs of consecutive characters or words, so for example in the case of word bi-grams, every consecutive pair of words would be a feature. Support for n-grams can be enabled by adjusting the `ngram_range` variable during the initialisation of the `CountVectorizer`.\n",
    "\n",
    "In the initialisation of `CountVectorizer` set the `ngram_range` variable to `(1, 2)`, and check the resulting `toy_count` matrix by running `toy_count.toarray()`. Do the results make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the columns can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the vectoriser is fitted, you can retrieve the index (starting from zero) of a particular word in the dictionary by simply calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer.vocabulary_.get(u'machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details about the functionality of `CountVectorizer`, please refer [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a small part of the comments (i.e the first five in the list) that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# The lines below load the TweetTokenizer from the nltk library.\n",
    "# You can comment-them-in along with the tokenizer variable of\n",
    "# the CountVectorizer should you like to see the results with\n",
    "# a different tokeniser.\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "# word_tokenizer = TweetTokenizer(preserve_case=False, \n",
    "#                                 strip_handles=True, \n",
    "#                                 reduce_len=True).tokenize\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                                   stop_words=None,\n",
    "                                   # tokenizer=word_tokenizer,\n",
    "                                   # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   max_df=1.0,\n",
    "                                   # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                   # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                   min_df=1)\n",
    "count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits and tranforms the corpus in its bag-of-words representation.\n",
    "X_train_count = count_vectorizer.fit_transform(input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the fitting process each term is assigned a unique integer index corresponding to a column in the resulting `X_train_count` matrix (i.e. equivalent to the $X$ matrix that has been mentioned in the description of this section of the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_count.shape)\n",
    "print(X_train_count.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell how many documents and how many unique words exist in the Kaggle dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Occurrences To Frequencies\n",
    "Occurrence count is a good start. However, longer documents will have higher average count values than shorter documents, even though they might talk about similar topics. To avoid these potential discrepancies, it suffices to divide the number of occurrences of each word in a document by the total number of words in the document. The number of times a term occurs in a document, divided by the number of terms in a document is called the **term frequency** (**tf**).\n",
    "\n",
    "Another refinement on top of term frequency is to downscale weights for words that occur in many documents in the corpus, and are therefore less informative than those that occur only in a smaller portion of the corpus. In order to achieve this we can weight terms on the basis of the **inverse document frequency** (**idf**). The *document frequency* is the number of documents a given word occurs in; the inverse document frequency is often defined as the total number of documents in the corpus divided by the document frequency.\n",
    "\n",
    "Combining tf and idf results in a *family of weightings* (tf is usually multiplied by idf, but there a few different variations of how idf is computed) known as **term frequency-inverse document frequency** (**tf–idf**).\n",
    "\n",
    "Both tf and tf–idf on our `toy_corpus` can be computed using `scikit-learn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the entirety of our toy corpus.\n",
    "toy_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Computing tf using the counts that have been computed from the CountVectorizer.\n",
    "tf_transformer = TfidfTransformer(use_idf=False, norm='l1', smooth_idf=False)\n",
    "X_train_tf = tf_transformer.fit_transform(toy_count)\n",
    "\n",
    "print(X_train_tf.shape)\n",
    "print(X_train_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing tf-idf using the counts that have been computed from the CountVectorizer.\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, norm='l1', smooth_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(toy_count)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than transforming the raw counts with the `TfidfTransformer`, it is alternatively possible to use the `TfidfVectorizer` to directly parse the dataset. We will use this to compute the tf-idf scores on our `toy_corpus` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                             # stop_words='english',\n",
    "                             # tokenizer='word_tokenizer',\n",
    "                             # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                             # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                             # max_df=0.75,\n",
    "                             # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                             # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                             # min_df=5,\n",
    "                             # tf-idf hyper-parameters\n",
    "                             use_idf=True, norm='l1', smooth_idf=False)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(toy_corpus)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the scores are identical to the ones computed at the previous step using the combination of `CountVectorizer` and `TfidfTransformer`. We will compute now the tf-idf scores on the Kaggle dataset using again the `TfidfVectorizer`.\n",
    "\n",
    "We are also leveraging the `stop_words`,`max_df` and `min_df` parameters of the `TfidfVectorizer` in order to exclude frequent and extremely infrequent words that would not help us in our classification task. This also helps us to reduce the number of columns of the $X$ matrix (i.e. `X_train_tfidf`) to only 1853 columns from the total of 14315 that it originally had when these parameters were not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                             stop_words='english',\n",
    "                             # tokenizer='word_tokenizer',\n",
    "                             # Ignore terms that have a document frequency strictly higher than the given threshold.\n",
    "                             # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                             max_df=0.75,\n",
    "                             # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                             # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                             min_df=5,\n",
    "                             # tf-idf hyper-parameters\n",
    "                             use_idf=True, norm='l1', smooth_idf=False)\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(input_train)\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try filtering out terms that are either too frequent or infrequent in the dataset by adjusting the `max_df` and `min_df` variable respectively. This is an easy way of not only filtering out the less informative words but also reducing the number of features (less storage complexity).\n",
    "\n",
    "Occasionally, it is important to have an understanding of the words that are excluded via the `stop_words` parameter—some of those words could be important for a particular task. The full list of `stop_words` that `sklearn` uses can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "for word in sorted(ENGLISH_STOP_WORDS): print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Predictive Model using K-Nearest-Neighbours\n",
    "Now that we have our training features and the labels of each post, we can train a classifier to predict whether a message is a spam or not. Let's start with a KNN classifier, which provides a simple baseline, although is perhaps not the best classifier for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=3).fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the outcome on a new comment, we need to extract the features using almost the same feature extracting chain as before. The differences are that we call (i) `transform` instead of `fit_transform` on the transformer or vectoriser, and (ii) `predict` on the classifier since they have both been fit to the training set.\n",
    "\n",
    "You can test your own comments by changing the text in the `test_comment` variable. Does your classifier identify spam messages properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = 'TO STOP THIS TEXT CALL 52331'\n",
    "test_comment_tfidf = tfidf_vect.transform([test_comment])\n",
    "y_pred = knn_clf.predict(test_comment_tfidf)\n",
    "\n",
    "print('%s: %s' % (test_comment, class_names[y_pred[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Performance on the Test Set\n",
    "We will be evaluating the performance of our KNN classifier on the *unseen* data of the test set based on the accuracy metric. In a binary classification task, such as ours, the accuracy with which a model predicts a specific class $c$ (e.g. spam) is formally defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\sum \\text{TP} + \\sum \\text{TN}}{\\sum \\text{TP} + \\sum \\text{FP} + \\sum \\text{TN} + \\sum \\text{FN}}\n",
    "\\end{align}\n",
    "where:\n",
    "* $\\text{TP}$ refers to True Positive predictions: both the predicted and the empirical labels are $c$\n",
    "* $\\text{TN}$ refers to True Negative predictions: both the predicted and the empirical labels are $\\neq c$\n",
    "* $\\text{FP}$ refers to False Positive predictions: the predicted label is $c$ but the empirical label $\\neq c$\n",
    "* $\\text{FN}$ refers to False Negative predictions: the predicted label is $\\neq c$ but the empirical label is $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "X_test_tfidf = tfidf_vect.transform(input_test)\n",
    "y_pred = knn_clf.predict(X_test_tfidf)\n",
    "print('Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Pipeline\n",
    "In order to make our pipeline (i.e. vectoriser or transformer $\\rightarrow$ classifier) easier to work with, `scikit-learn` provides the `Pipeline` class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipeline = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                               stop_words='english',\n",
    "                                               # tokenizer=word_tokenizer,\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               max_df=1.0,\n",
    "                                               # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               min_df=5,\n",
    "                                               # tf-idf hyper-parameters\n",
    "                                               use_idf=True, norm='l1', smooth_idf=False)),\n",
    "                         ('clf', KNeighborsClassifier(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names `tfidf` and `clf` (classifier) are arbitrary. We shall see their use in the section on grid search, below. We can now train (on the training set) and test (on the test set) the model in a similar fashion to when we had all the different components separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "clf_pipeline.fit(input_train, y_train)\n",
    "y_pred = clf_pipeline.predict(input_test) # We are making prediction on the test set.\n",
    "print('Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better with a linear Support Vector Machine (SVM). We can change the learner by just plugging a different classifier object into our pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_pipeline = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                               stop_words='english',\n",
    "                                               # tokenizer=word_tokenizer,\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               max_df=1.0,\n",
    "                                               # Ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "                                               # If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "                                               min_df=5,\n",
    "                                               # tf-idf hyper-parameters\n",
    "                                               use_idf=True, norm='l2', smooth_idf=False)),\n",
    "                         ('clf', SGDClassifier(loss='hinge',\n",
    "                                           penalty='l2',\n",
    "                                           tol=1e-5,\n",
    "                                           random_state=random_state))])\n",
    "# Model Training\n",
    "clf_pipeline.fit(input_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_pipeline.predict(input_test) # We are making prediction on the test set.\n",
    "print('Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "`scikit-learn` provides further utilities for a more detailed performance analysis of the results using different metrics (i.e. precision, recall, and F1-score).\n",
    "\n",
    "A *confusion matrix* is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. An example of a binary confusion matrix is presented below:\n",
    "\n",
    "| Class          | <span style=\"font-weight:normal\">Predicted: `ham`</span> | <span style=\"font-weight:normal\">Predicted: `spam`</span> |\n",
    "|--------------------|------------------|-------------------|\n",
    "| Actual: `ham`  | TN               | FP                |\n",
    "| Actual: `spam` | FN               | TP                |\n",
    "\n",
    "The table above illustrates the different classification scenarios:\n",
    "* $\\text{TP}$ refers to True Positive predictions: the email is a spam and the algorithm predicted spam\n",
    "* $\\text{TN}$ refers to True Negative predictions: the email is a ham and the algorithm predicted ham\n",
    "* $\\text{FP}$ refers to False Positve predictions: the email is a ham and the algorithm predicted spam \n",
    "* $\\text{FN}$ refers to False Negative predictions: the email is a spam and the algorithm predicted ham\n",
    "\n",
    "A number of performance metrics, complimentary to the accuracy that we looked at above, can be derived from the confusion matrix:\n",
    " \n",
    "* **Precision** measures the ratio of correctly predicted topic to the total, and is formally defined as:\n",
    "\\begin{align}\n",
    "\\text{precision}=\\frac{\\sum \\text{TP}}{\\sum \\text{TP}+ \\sum\\text{FP}}\n",
    "\\end{align}\n",
    " \n",
    "\n",
    "* **Recall** measures the ratio of correctly predicted topic to the total, and is formally defined as:\n",
    "\\begin{align}\n",
    "\\text{recall}=\\frac{\\sum \\text{TP}}{\\sum \\text{TP}+ \\sum \\text{FN}}\n",
    "\\end{align}\n",
    " \n",
    "\n",
    "- **F1-score** is a metric that jointly optimises the precision and recall, and is formally defined as:\n",
    "\\begin{align}\n",
    "\\text{F1-score}=2\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(metrics.confusion_matrix(y_test,\n",
    "                                              y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output matrix shows the performance across all the different classes according to precision, recall and f1-score. Support refers to the number of samples that belong to each particular class.\n",
    "\n",
    "For further details you can have a look [here](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, \n",
    "                                    y_pred,\n",
    "                                    target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did before, we can test how well our classifier is doing by inputing our own comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = 'Text 12312 NOW to get this offer!'\n",
    "y_pred = clf_pipeline.predict([test_comment])\n",
    "\n",
    "print('%s: %s' % (test_comment, class_names[y_pred[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try experimenting with different hyper-parameters (e.g. `ngram_range`, `tokenizer`, `max_df` or `min_df`) to see whether you achieve any better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning using Grid Search and Cross-validation\n",
    "We have already encountered some hyper-parameters such as `use_idf` in the `TfidfTransformer` (and `TfidfVectorizer`). Classifiers tend to have many hyper-parameters as well. For example `KNeighborsClassifier` includes parameter for the number of neighbours and `SGDClassifier` has a penalty parameter alpha and configurable loss and penalty terms in the objective function.\n",
    "\n",
    "Instead of tweaking the hyper-parameters of the various components of the chain, it is possible to run an exhaustive search of the best hyper-parameters on a grid of possible values. Let's use this to explore whether we can make the KNeighborsClassifier perform as well as our linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('clf', KNeighborsClassifier(n_neighbors=3))])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__n_neighbors': (1, 3, 5, 7, 9)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "$k$-fold cross-validation is the process of splitting the training data into $k$ smaller sets, and training a model with the same hyper-parameters $k$ times. During each one of the training phases, a different part of the $k$ sets is left out and the model is trained using the rest $k-1$ folds of the data. After a single training phase is completed, the model is validated on the single fold which it had not seen during its training.\n",
    "![Cross-validation](Figures/grid_search_cross_validation.png \"How cross-validation works?\")\n",
    "Source: [https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of $k$ folds that will be used for cross-validation is determined using the `cv` parameter on the `GridSearchCV` module of the `sklearn` library.\n",
    "\n",
    "Obviously, such an exhaustive search can be expensive. If we have multiple CPU cores at our disposal, we can tell the grid searcher to try these eleven parameter combinations in parallel with the `n_jobs` parameter. If we give this parameter a value of -1, grid search will detect how many cores are installed and uses them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline = GridSearchCV(knn_pipeline, param_grid=parameters, \n",
    "                            # Sets the number of folds (k) that will be used for \n",
    "                            # k-fold cross-validation.\n",
    "                            cv=3, \n",
    "                            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search instance behaves like a normal `scikit-learn` model, so we can use the `fit` function to initialise the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline.fit(input_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the optimal parameters out by inspecting the object's `grid_scores_` attribute, which is a list of parameters/score pairs. To get the best scoring attributes, we do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in knn_pipeline.best_params_:\n",
    "    print(\"%s: %r\" % (param_name, knn_pipeline.best_params_[param_name]))\n",
    "print('The best achieved cross-validation accuracy is %.2f' % knn_pipeline.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the process is complete, we will test our performance on the test set, and output the corresponding confusion matrix with the precision, recall and f1-score across the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_pipeline.predict(input_test) # We are making prediction on the test set.\n",
    "print('Accuracy: %.2f' % (metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, \n",
    "                                    y_pred,\n",
    "                                    target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test and see how well new comments are classified on our `knn_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = 'Send code if you want to buy this amazing new application!'\n",
    "y_pred = knn_pipeline.predict([test_comment])\n",
    "\n",
    "print('%s: %s' % (test_comment, class_names[y_pred[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring K-Means Clustering\n",
    "Now that we have extracted features from our training documents we're in a position to experiment with clustering. We will use K-Means as its one of the most intuitive clustering methods, although it does have a few limitations.\n",
    "\n",
    "K-Means clustering with 10 clusters can be achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "k_means = KMeans(num_clusters)\n",
    "k_means.fit(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignments of the original posts to cluster id is given by `km.labels_` once `km.fit(...)` has been called. The centroids of the clusters is given by `km.cluster_centers_`. Intuitively, the vector that describes the centre of a cluster is just like any other feature vector. An interesting way to explore what each cluster is representing is to calculate and print the top weighted (either by occurrence or tf-idf) terms for that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = k_means.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vect.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :5]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of different metrics exist that allow us to measure how well the clusters fit the known distribution of underlying newsgroups. One such metric is the homogeneity which is a measure of how pure the clusters are with respect to the known labels (i.e. spam or ham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Homogeneity: %.3f\" % metrics.homogeneity_score(y_train, k_means.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity scores vary between 0 and 1; a score of 1 indicates that the clusters match the original label distribution exactly.\n",
    "\n",
    "Explore what happens if you make the number of clusters larger. What do you notice? Do the clusters begin to make more intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have enjoyed this tutorial, the exercises in [Working With Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) are a good next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
